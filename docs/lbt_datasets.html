<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>libratools.lbt_datasets API documentation</title>
<meta name="description" content="The libratools.lbt_datasets module includes utilities to load, manipulate
and save trajectory datasets." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}
    {display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>libratools.lbt_datasets</code></h1>
</header>
<section id="section-intro">
<p>The libratools.lbt_datasets module includes utilities to load, manipulate
and save trajectory datasets.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#!/usr/bin/env python
# -*- coding: utf-8 -*-

&#34;&#34;&#34;
The libratools.lbt_datasets module includes utilities to load, manipulate 
and save trajectory datasets.
&#34;&#34;&#34;

import os     # standard library
import yaml
import pathlib
import configparser

import locale    # 3rd party packages
import numpy as np
import pandas as pd


__author__ = &#34;Vincent (Vince) J. Straub&#34;
__email__ = &#34;vincejstraub@gmail.com&#34;
__status__ = &#34;Testing&#34;


# set config file depending on whether process.py or app.py is being run
if pathlib.Path.cwd().name is &#39;Processing&#39;:
    CONFIG_PATH = pathlib.Path.cwd() / &#39;./libratools/libratools/config.ini&#39;
elif pathlib.Path.cwd().name is &#39;DevExDashboard&#39;:
    CONFIG_PATH = pathlib.Path.cwd().parents[1] / &#39;./libratools/libratools/config.ini&#39;


class configReader:
    __conf = None

    @staticmethod
    def config():
        if configReader.__conf is None:  # read only once, lazy
            configReader.__conf = configparser.ConfigParser()
            configReader.__conf.read(CONFIG_PATH)
        return configReader.__conf


# read directory configuration for global vars
BIOTRACKER_COLS = configReader.config()[&#39;VARS&#39;][&#39;BIOTRACKER_COLS&#39;].split(&#39;,\n&#39;)


def check_columns(df, columns=BIOTRACKER_COLS):
    &#34;&#34;&#34;
    Checks there are expected number of columns, returns dataframe
    with columns redefined and index reset if not.

    Args:
        df (pandas.DataFrame): dataframe to check.
        col_num (int, default=14): used as reference for number of
            columns that should be in dataframe.
        columns (list, default=BIOTRACKER_COLS): list of reference columns
            that should be in dataframe.
    &#34;&#34;&#34;
    if list(df.columns) != columns:
        df = df.reset_index()
        df.columns = columns
    else:
        pass
    return df


def load_trajectory(file_path, dropna=False, na_summary=True, skiprows=3,
                    warn_bad_lines=True, sep=&#39;;&#39;, cols=BIOTRACKER_COLS,
                    keycols=[&#39;FRAME&#39;, &#39;x&#39;, &#39;y&#39;]):
    &#34;&#34;&#34;
    Loads a CSV file generated from BioTracker using Pandas and numpy.
    Note that the first 3 rows containing metadata and lines with too
    many commas are automatically dropped. Whether to drop rows with
    missing values (NaN) is left up to the user.

    Args:
        file_path (string): path to file.
        drop_na (bool, default=False): if drop_na=False NaN rows are kept, if
            drop_na=True NaN rows are dropped (where at least one element is
            missing)
        na_summary (bool, default=True): if na_summary=True, the number of
            rows dropped is displayed as an int.
        skiprows (int): number of rows to skip.
        warn_bad_lines (bool, default=True): If error_bad_lines is False,
            and warn_bad_lines is True, a warning for each “bad line” will
            be output.
        sep (str, default=&#39;;&#39;): seperator to use.
        cols (list, default=BIOTRACKER_COLS): list of expected column values.
        keycols (list, default=[&#39;FRAME&#39;, &#39;x&#39;, &#39;y&#39;]): list of key columns to 
            check for missing values.

    Returns:
        A pandas.DataFrame.
        A numpy array.
    &#34;&#34;&#34;
    # message to display if NaN values detected
    NA_MSG = &#39;Missing FRAME, x, and y values detected for file:\n&#39;

    # read csv file
    df = pd.read_csv(file_path, skiprows=skiprows, delimiter=sep,
                     error_bad_lines=False, warn_bad_lines=warn_bad_lines)

    # check columns exist
    df = check_columns(df, columns=cols)

    # check for missing values in key columns
    if df[keycols].isna().sum().any() is True:
        # store info on rows with missing values
        num_na_rows = np.count_nonzero(df[keycols].isna())
        # decide whether to drop rows and display summary info
        if dropna and na_summary is True:
            print(NA_MSG + file_path)
            df = df.dropna()
            print(&#39;Rows dropped: {}.\n&#39;.format(num_na_rows))
        elif dropna is False and na_summary is True:
            print(NA_MSG + file_path)
            print(&#39;Rows with missing values: {}.\n&#39;.format(num_na_rows))
        elif dropna is True and na_summary is False:
            df = df.dropna()

        else:
            pass
    else:
        pass

    # convert timeString column to datetime
    date_time_format = &#39;%a %b %d %H:%M:%S %Y&#39;
    # set locale to German time for converting timeString column
    locale.setlocale(locale.LC_ALL, (&#39;de&#39;, &#39;utf-8&#39;))
    df[&#39;timeString&#39;] = pd.to_datetime(df[&#39;timeString&#39;], errors=&#39;ignore&#39;,
                                      format=date_time_format)
    # convert DataFrame to numpy array
    data = df.to_numpy()

    return df, data


def load_npz(file_path, array=&#39;&#39;):
    &#34;&#34;&#34;
    Loads NPZ file from disk and returns as numpy array, optionally
    returning a single array.

    Args:
        file_path (str): path to file.
        array (str, default=&#39;&#39;): array key to index NPZ file, if
            array=&#39;&#39; NPZ object is returned.

    Returns:
        f (numpy array).
    &#34;&#34;&#34;
    f = np.load(file_path)
    if array != &#39;&#39;:
        try:
            data = f[array]
            return data
        except KeyError:
            print(f&#39;{array} is not a key in the NPZ file.&#39;)
    else:
        return f


def read_file_paths(indir=&#39;cwd&#39;, extension=&#39;.csv&#39;, warning=True, suffix=False,
                    suffix_str=&#39;&#39;):
    &#34;&#34;&#34;
    Stores paths of files with specified file type in a list; first looks 
    in current directory before asking user to provide alternative directory
    path if none are found.

    Args:
        indir (str, default=&#39;cwd&#39;): input directory containing files.
        extension (str, default=&#39;csv&#39;): file extension.
        warning (bool default=True): if warning=True, an informational 
            message is displayed in case no files are found.
        suffix (bool default=False): if suffix=True, only file paths 
            ending in suffix_str are returned.
        suffix_str (str, &#39;&#39;): suffix.

    Returns:
        A list of file paths.
    &#34;&#34;&#34;
    # search for files in current directory if no indir is provided
    extension = extension.lower()
    extension_cap = extension.capitalize()
    if indir == &#39;cwd&#39;:
        file_paths = [p for p in indir.rglob(f&#39;*{extension}&#39;)]
        num_files = len(file_paths)

        # prompt user for directory path if no none found else store paths
        if num_files == 0:
            print(f&#39;No {extension_cap} files found in current working directory&#39;)

    # check provided directory path exists and read files
    else:
        assert os.path.exists(indir), &#39;Directory path not found.&#39;
        file_paths = [p for p in indir.rglob(f&#39;*{extension}&#39;)]

        # check CSV files exist
        num_files = len(file_paths)
        if num_files == 0 and warning is True:
            print(f&#39;No {extension_cap} files found in {indir}.&#39;)

        if suffix is True:
            files_dir = pathlib.Path(file_paths[0]).parents[0]
            file_stems = [pathlib.Path(p).stem for p in file_paths]
            files = [f + extension for f in file_stems if
                    os.path.splitext(f)[1][-len(suffix_str):] == suffix_str]
            file_paths = [files_dir / f for f in files]

    return file_paths, num_files


def find_dir(path=&#39;cwd&#39;, prefix=&#39;&#39;, suffix=&#39;&#39;):
    &#34;&#34;&#34;
    Returns subdirectory path that begins and ends with specific strings by
    using os.walk() to search through all directory and file paths in root
    directory of current working directory.
    Args:
        path (str, default=cwd): directory to search, if default=cwd,
            the current working directory is searched.
        prefix (str, default=&#39;&#39;): prefix of subdirectory path.
        suffix (str, default=&#39;&#39;): suffix of subdirectory path.
    Returns:
        dir_path (str).
    &#34;&#34;&#34;
    prefix_str = str(prefix)
    suffix_str = str(suffix)
    date_dir = None
    if path == &#39;cwd&#39;:
        path = pathlib.Path.cwd()
    for root, dirs, files in os.walk(path):
        for dir in dirs:
            if dir.startswith(f&#34;{prefix_str}&#34;) and dir.endswith(f&#34;{suffix_str}&#34;):
                date_dir = dir

    if date_dir is None:
        print(f&#39;Directory ending with {suffix} not found.&#39;)
    else:
        return date_dir


def list_dirs(parent_dir_path):
    &#34;&#34;&#34;
    Returns all child directory names in parent directory as a list.
    Args:
        parent_dir_path (str): path to directory containing subdirectories.
    &#34;&#34;&#34;
    return [d for d in os.listdir(parent_dir_path) if
            os.path.isdir(pathlib.Path(parent_dir_path, d))]


def read_subdir_paths(parent_dir=&#39;&#39;):
    &#34;&#34;&#34;
    Returns directory paths for all subdirectories in provided directory
    path as strings in a list, and number of subdirectories as an integer.S
    Args:
        parent_dir (str, default=&#39;&#39;): directory in which to locate all
            subdirectories, defaults to current working directory if not
            path is provided.
    &#34;&#34;&#34;
    if parent_dir == &#39;&#39;:
        parent_dir = pathlib.Path.cwd()

    # locate subdirectories
    try:
        subdirs = [parent_dir + file for file in list_dirs(parent_dir)]
    except ValueError:
        print(&#39;Provided directory path not found.&#39;)
    # store directory count as number of recordings
    num_dirs = len(set(subdirs))

    # load each file
    file_paths = []
    for subdir in subdirs:
        file_path, _ = read_file_paths(indir=subdir)
        for file in file_path:
            file_paths.append(file)

    return file_paths, num_dirs


def read_metadata(file_path, num_comment_lines=3):
    &#34;&#34;&#34;
    Returns metadata stored as comments in the first few lines of a
    BioTracker-generated CSV file as list where each comment is an item
    stored as a string.

    Args:
        file_path (str): path to BioTracker-generated CSV file.
        num_comment_lines (int, default=3): number of lines at the beginning
            of file that contain comments.
    &#34;&#34;&#34;
    with open(file_path) as file:
        # Read specified number of lines
        metadata = [file.readline() for line in range(num_comment_lines)]

    return metadata


def extract_comments_as_dict(dic):
    &#34;&#34;&#34;
    Takes list of key-value pair comments and returns dict by 
    splitting on standard python chars # and \n.
    &#34;&#34;&#34;
    comments = [comment.split(&#39;#&#39;)[1].strip() for comment in dic]
    keys = [val.split(&#39;:&#39;)[0] for val in comments]
    keys_comments = dict(zip(keys, comments))
    values = [k.replace(j + &#39;:&#39;, &#39;&#39;).strip() for j, k in keys_comments.items()]
    dic = dict(zip(keys, values))
    for key in dic.keys():
        dic[key] = dic[key].strip()
        try:
            dic[key] = float(dic[key])
        except ValueError:
            pass

        return dic


def get_chunk_number_from_path(file_path, dir_sep=&#39;/&#39;, subdir_sep=&#39;.&#39;,
                               as_str=False):
    &#34;&#34;&#34;
    Returns last two characters of file path corresponding to chunk number.

    Args:
        file_path (str): path to file.
        dir_sep (str, default=&#39;/&#39;): character that separates directories and
            files in file path.
        subdir_sep (str, default=&#39;.&#39;): character that further separates
            directories and files in file path.
        as_str (bool, default=True): if as_str=True, chunk number is returned
            as string.
    &#34;&#34;&#34;
    # split file path up using &#39;/&#39; and &#39;.&#39; separator
    _chunk_num = pathlib.Path(file_path).stem.split(f&#39;{dir_sep}&#39;)[-1].split(f&#39;{subdir_sep}&#39;)
    try:
        chunk_num = int(_chunk_num[0])
        if as_str is True:
            return str(chunk_num)
        else:
            return chunk_num
    except ValueError:
        print(f&#39;Chunk value {_chunk_num[0]} is not an integer value.&#39;)


def save_trajectory_to_csv(df, f_name=&#39;&#39;, outdir=&#39;&#39;, metadata=&#39;&#39;, 
                           extension=&#39;.csv&#39;, save_msg=True, add_metadata=True, 
                           suffix=&#39;_processed&#39;):
    &#34;&#34;&#34;
    Saves pandas.DataFrame object as a CSV file and prepends any metadata
    provided in a file object.
    Args:
        df (pandas.DataFrame): dataframe to be saved to file.
        metadata (list, str): metdata to be stored to file, can be either
            a string of a list of strings.
        f_name (str): file path.
        outdir: output directory.
        save_msg (bool, default=True): prints message to confirm track
            has been saved if save_msg=True.
        add_metadata (bool, default=True): adds metadata comments to file
            as header if add_metadata=Trueread_file_path.
        suffix (str, default=&#39;_processed&#39;): suffix to add to the end of file
            when saving.
    &#34;&#34;&#34;
    # save DataFrame to CSV
    
    f = pathlib.Path(f_name).stem + suffix + extension
    path = outdir / f
    df.to_csv(path, sep=&#39;,&#39;, encoding=&#39;utf-8&#39;,
              index=False)

    # optionally prepend metadata to CSV
    if add_metadata is True:
        prepend_comments_to_csv(path, metadata)

    # optionally confirm saving
    if save_msg:
        if suffix is &#39;_processed&#39;:
            print(f&#39;Processed {pathlib.Path(f_name).stem}{extension} and saved file to disk.\n&#39;)
        else:
            print(f&#39;Merged {pathlib.Path(f_name).stem}{extension} and saved file to disk.&#39;)
    else:
        pass


def prepend_comments_to_csv(file, comments, extension=&#39;.csv&#39;):
    &#34;&#34;&#34;
    Insert a list of strings as a new lines at the beginning of a CSV file.
    &#34;&#34;&#34;
    # define name of temporary dummy file
    file_path = pathlib.Path(file).parents[0]
    file_name = pathlib.Path(file).stem
    temp_file = file_path / (file_name + &#39;.bak&#39;)
    # open given original file in read mode  and dummy file in write mode
    with open(file, &#39;r&#39;) as read_obj, open(temp_file, &#39;w&#39;) as write_obj:
        # iterate over list of comments and write them to dummy file as lines
        for line in comments:
            write_obj.write(line)
        # read lines from original file and append them to the dummy file
        for line in read_obj:
            write_obj.write(line)
    # remove original file
    file.unlink()
    # rename dummy file as the original file
    new_extension = temp_file.with_suffix(extension)
    temp_file.rename(new_extension)

    
def read_yaml_as_dict(path):
    &#34;&#34;&#34;
    Returns yaml dict values.
    &#34;&#34;&#34;
    with open(path, &#39;r&#39;) as stream:
        try:
            dic = yaml.safe_load(stream)
        except yaml.YAMLError as exc:
            print(exc)

    return dic


def dict_to_comments(dic, sep=&#39;: &#39;):
    &#34;&#34;&#34;
    Takes dictionary key, value paris and returns list of comments.
    &#34;&#34;&#34;
    comments = [&#39;# &#39;+str(k)+sep+str(v)+&#39;\n&#39; for k, v in dic.items()]

    return comments


</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="libratools.lbt_datasets.check_columns"><code class="name flex">
<span>def <span class="ident">check_columns</span></span>(<span>df, columns=['FRAME', 'MillisecsByFPS', 'objectName', 'valid', 'id', 'coordinateUnit', 'x', 'y', 'rad', 'deg', 'xpx', 'ypx', 'time', 'timeString'])</span>
</code></dt>
<dd>
<div class="desc"><p>Checks there are expected number of columns, returns dataframe
with columns redefined and index reset if not.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>dataframe to check.</dd>
<dt><strong><code>col_num</code></strong> :&ensp;<code>int</code>, default=<code>14</code></dt>
<dd>used as reference for number of
columns that should be in dataframe.</dd>
<dt><strong><code>columns</code></strong> :&ensp;<code>list</code>, default=<code>BIOTRACKER_COLS</code></dt>
<dd>list of reference columns
that should be in dataframe.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def check_columns(df, columns=BIOTRACKER_COLS):
    &#34;&#34;&#34;
    Checks there are expected number of columns, returns dataframe
    with columns redefined and index reset if not.

    Args:
        df (pandas.DataFrame): dataframe to check.
        col_num (int, default=14): used as reference for number of
            columns that should be in dataframe.
        columns (list, default=BIOTRACKER_COLS): list of reference columns
            that should be in dataframe.
    &#34;&#34;&#34;
    if list(df.columns) != columns:
        df = df.reset_index()
        df.columns = columns
    else:
        pass
    return df</code></pre>
</details>
</dd>
<dt id="libratools.lbt_datasets.dict_to_comments"><code class="name flex">
<span>def <span class="ident">dict_to_comments</span></span>(<span>dic, sep=': ')</span>
</code></dt>
<dd>
<div class="desc"><p>Takes dictionary key, value paris and returns list of comments.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dict_to_comments(dic, sep=&#39;: &#39;):
    &#34;&#34;&#34;
    Takes dictionary key, value paris and returns list of comments.
    &#34;&#34;&#34;
    comments = [&#39;# &#39;+str(k)+sep+str(v)+&#39;\n&#39; for k, v in dic.items()]

    return comments</code></pre>
</details>
</dd>
<dt id="libratools.lbt_datasets.extract_comments_as_dict"><code class="name flex">
<span>def <span class="ident">extract_comments_as_dict</span></span>(<span>dic)</span>
</code></dt>
<dd>
<div class="desc"><p>Takes list of key-value pair comments and returns dict by
splitting on standard python chars # and
.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract_comments_as_dict(dic):
    &#34;&#34;&#34;
    Takes list of key-value pair comments and returns dict by 
    splitting on standard python chars # and \n.
    &#34;&#34;&#34;
    comments = [comment.split(&#39;#&#39;)[1].strip() for comment in dic]
    keys = [val.split(&#39;:&#39;)[0] for val in comments]
    keys_comments = dict(zip(keys, comments))
    values = [k.replace(j + &#39;:&#39;, &#39;&#39;).strip() for j, k in keys_comments.items()]
    dic = dict(zip(keys, values))
    for key in dic.keys():
        dic[key] = dic[key].strip()
        try:
            dic[key] = float(dic[key])
        except ValueError:
            pass

        return dic</code></pre>
</details>
</dd>
<dt id="libratools.lbt_datasets.find_dir"><code class="name flex">
<span>def <span class="ident">find_dir</span></span>(<span>path='cwd', prefix='', suffix='')</span>
</code></dt>
<dd>
<div class="desc"><p>Returns subdirectory path that begins and ends with specific strings by
using os.walk() to search through all directory and file paths in root
directory of current working directory.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>path</code></strong> :&ensp;<code>str</code>, default=<code>cwd</code></dt>
<dd>directory to search, if default=cwd,
the current working directory is searched.</dd>
</dl>
<p>prefix (str, default=''): prefix of subdirectory path.
suffix (str, default=''): suffix of subdirectory path.</p>
<h2 id="returns">Returns</h2>
<p>dir_path (str).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_dir(path=&#39;cwd&#39;, prefix=&#39;&#39;, suffix=&#39;&#39;):
    &#34;&#34;&#34;
    Returns subdirectory path that begins and ends with specific strings by
    using os.walk() to search through all directory and file paths in root
    directory of current working directory.
    Args:
        path (str, default=cwd): directory to search, if default=cwd,
            the current working directory is searched.
        prefix (str, default=&#39;&#39;): prefix of subdirectory path.
        suffix (str, default=&#39;&#39;): suffix of subdirectory path.
    Returns:
        dir_path (str).
    &#34;&#34;&#34;
    prefix_str = str(prefix)
    suffix_str = str(suffix)
    date_dir = None
    if path == &#39;cwd&#39;:
        path = pathlib.Path.cwd()
    for root, dirs, files in os.walk(path):
        for dir in dirs:
            if dir.startswith(f&#34;{prefix_str}&#34;) and dir.endswith(f&#34;{suffix_str}&#34;):
                date_dir = dir

    if date_dir is None:
        print(f&#39;Directory ending with {suffix} not found.&#39;)
    else:
        return date_dir</code></pre>
</details>
</dd>
<dt id="libratools.lbt_datasets.get_chunk_number_from_path"><code class="name flex">
<span>def <span class="ident">get_chunk_number_from_path</span></span>(<span>file_path, dir_sep='/', subdir_sep='.', as_str=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns last two characters of file path corresponding to chunk number.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>file_path</code></strong> :&ensp;<code>str</code></dt>
<dd>path to file.</dd>
<dt>dir_sep (str, default='/'): character that separates directories and</dt>
<dt>files in file path.</dt>
<dt>subdir_sep (str, default='.'): character that further separates</dt>
<dt>directories and files in file path.</dt>
<dt><strong><code>as_str</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>if as_str=True, chunk number is returned
as string.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_chunk_number_from_path(file_path, dir_sep=&#39;/&#39;, subdir_sep=&#39;.&#39;,
                               as_str=False):
    &#34;&#34;&#34;
    Returns last two characters of file path corresponding to chunk number.

    Args:
        file_path (str): path to file.
        dir_sep (str, default=&#39;/&#39;): character that separates directories and
            files in file path.
        subdir_sep (str, default=&#39;.&#39;): character that further separates
            directories and files in file path.
        as_str (bool, default=True): if as_str=True, chunk number is returned
            as string.
    &#34;&#34;&#34;
    # split file path up using &#39;/&#39; and &#39;.&#39; separator
    _chunk_num = pathlib.Path(file_path).stem.split(f&#39;{dir_sep}&#39;)[-1].split(f&#39;{subdir_sep}&#39;)
    try:
        chunk_num = int(_chunk_num[0])
        if as_str is True:
            return str(chunk_num)
        else:
            return chunk_num
    except ValueError:
        print(f&#39;Chunk value {_chunk_num[0]} is not an integer value.&#39;)</code></pre>
</details>
</dd>
<dt id="libratools.lbt_datasets.list_dirs"><code class="name flex">
<span>def <span class="ident">list_dirs</span></span>(<span>parent_dir_path)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns all child directory names in parent directory as a list.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>parent_dir_path</code></strong> :&ensp;<code>str</code></dt>
<dd>path to directory containing subdirectories.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def list_dirs(parent_dir_path):
    &#34;&#34;&#34;
    Returns all child directory names in parent directory as a list.
    Args:
        parent_dir_path (str): path to directory containing subdirectories.
    &#34;&#34;&#34;
    return [d for d in os.listdir(parent_dir_path) if
            os.path.isdir(pathlib.Path(parent_dir_path, d))]</code></pre>
</details>
</dd>
<dt id="libratools.lbt_datasets.load_npz"><code class="name flex">
<span>def <span class="ident">load_npz</span></span>(<span>file_path, array='')</span>
</code></dt>
<dd>
<div class="desc"><p>Loads NPZ file from disk and returns as numpy array, optionally
returning a single array.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>file_path</code></strong> :&ensp;<code>str</code></dt>
<dd>path to file.</dd>
</dl>
<p>array (str, default=''): array key to index NPZ file, if
array='' NPZ object is returned.</p>
<h2 id="returns">Returns</h2>
<p>f (numpy array).</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_npz(file_path, array=&#39;&#39;):
    &#34;&#34;&#34;
    Loads NPZ file from disk and returns as numpy array, optionally
    returning a single array.

    Args:
        file_path (str): path to file.
        array (str, default=&#39;&#39;): array key to index NPZ file, if
            array=&#39;&#39; NPZ object is returned.

    Returns:
        f (numpy array).
    &#34;&#34;&#34;
    f = np.load(file_path)
    if array != &#39;&#39;:
        try:
            data = f[array]
            return data
        except KeyError:
            print(f&#39;{array} is not a key in the NPZ file.&#39;)
    else:
        return f</code></pre>
</details>
</dd>
<dt id="libratools.lbt_datasets.load_trajectory"><code class="name flex">
<span>def <span class="ident">load_trajectory</span></span>(<span>file_path, dropna=False, na_summary=True, skiprows=3, warn_bad_lines=True, sep=';', cols=['FRAME', 'MillisecsByFPS', 'objectName', 'valid', 'id', 'coordinateUnit', 'x', 'y', 'rad', 'deg', 'xpx', 'ypx', 'time', 'timeString'], keycols=['FRAME', 'x', 'y'])</span>
</code></dt>
<dd>
<div class="desc"><p>Loads a CSV file generated from BioTracker using Pandas and numpy.
Note that the first 3 rows containing metadata and lines with too
many commas are automatically dropped. Whether to drop rows with
missing values (NaN) is left up to the user.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>file_path</code></strong> :&ensp;<code>string</code></dt>
<dd>path to file.</dd>
<dt><strong><code>drop_na</code></strong> :&ensp;<code>bool</code>, default=<code>False</code></dt>
<dd>if drop_na=False NaN rows are kept, if
drop_na=True NaN rows are dropped (where at least one element is
missing)</dd>
<dt><strong><code>na_summary</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>if na_summary=True, the number of
rows dropped is displayed as an int.</dd>
<dt><strong><code>skiprows</code></strong> :&ensp;<code>int</code></dt>
<dd>number of rows to skip.</dd>
<dt><strong><code>warn_bad_lines</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>If error_bad_lines is False,
and warn_bad_lines is True, a warning for each “bad line” will
be output.</dd>
<dt>sep (str, default=';'): seperator to use.</dt>
<dt><strong><code>cols</code></strong> :&ensp;<code>list</code>, default=<code>BIOTRACKER_COLS</code></dt>
<dd>list of expected column values.</dd>
</dl>
<p>keycols (list, default=['FRAME', 'x', 'y']): list of key columns to
check for missing values.</p>
<h2 id="returns">Returns</h2>
<p>A pandas.DataFrame.
A numpy array.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_trajectory(file_path, dropna=False, na_summary=True, skiprows=3,
                    warn_bad_lines=True, sep=&#39;;&#39;, cols=BIOTRACKER_COLS,
                    keycols=[&#39;FRAME&#39;, &#39;x&#39;, &#39;y&#39;]):
    &#34;&#34;&#34;
    Loads a CSV file generated from BioTracker using Pandas and numpy.
    Note that the first 3 rows containing metadata and lines with too
    many commas are automatically dropped. Whether to drop rows with
    missing values (NaN) is left up to the user.

    Args:
        file_path (string): path to file.
        drop_na (bool, default=False): if drop_na=False NaN rows are kept, if
            drop_na=True NaN rows are dropped (where at least one element is
            missing)
        na_summary (bool, default=True): if na_summary=True, the number of
            rows dropped is displayed as an int.
        skiprows (int): number of rows to skip.
        warn_bad_lines (bool, default=True): If error_bad_lines is False,
            and warn_bad_lines is True, a warning for each “bad line” will
            be output.
        sep (str, default=&#39;;&#39;): seperator to use.
        cols (list, default=BIOTRACKER_COLS): list of expected column values.
        keycols (list, default=[&#39;FRAME&#39;, &#39;x&#39;, &#39;y&#39;]): list of key columns to 
            check for missing values.

    Returns:
        A pandas.DataFrame.
        A numpy array.
    &#34;&#34;&#34;
    # message to display if NaN values detected
    NA_MSG = &#39;Missing FRAME, x, and y values detected for file:\n&#39;

    # read csv file
    df = pd.read_csv(file_path, skiprows=skiprows, delimiter=sep,
                     error_bad_lines=False, warn_bad_lines=warn_bad_lines)

    # check columns exist
    df = check_columns(df, columns=cols)

    # check for missing values in key columns
    if df[keycols].isna().sum().any() is True:
        # store info on rows with missing values
        num_na_rows = np.count_nonzero(df[keycols].isna())
        # decide whether to drop rows and display summary info
        if dropna and na_summary is True:
            print(NA_MSG + file_path)
            df = df.dropna()
            print(&#39;Rows dropped: {}.\n&#39;.format(num_na_rows))
        elif dropna is False and na_summary is True:
            print(NA_MSG + file_path)
            print(&#39;Rows with missing values: {}.\n&#39;.format(num_na_rows))
        elif dropna is True and na_summary is False:
            df = df.dropna()

        else:
            pass
    else:
        pass

    # convert timeString column to datetime
    date_time_format = &#39;%a %b %d %H:%M:%S %Y&#39;
    # set locale to German time for converting timeString column
    locale.setlocale(locale.LC_ALL, (&#39;de&#39;, &#39;utf-8&#39;))
    df[&#39;timeString&#39;] = pd.to_datetime(df[&#39;timeString&#39;], errors=&#39;ignore&#39;,
                                      format=date_time_format)
    # convert DataFrame to numpy array
    data = df.to_numpy()

    return df, data</code></pre>
</details>
<dt id="libratools.lbt_datasets.prepend_comments_to_csv"><code class="name flex">
<span>def <span class="ident">prepend_comments_to_csv</span></span>(<span>file, comments, extension='.csv')</span>
</code></dt>
<dd>
<div class="desc"><p>Insert a list of strings as a new lines at the beginning of a CSV file.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepend_comments_to_csv(file, comments, extension=&#39;.csv&#39;):
    &#34;&#34;&#34;
    Insert a list of strings as a new lines at the beginning of a CSV file.
    &#34;&#34;&#34;
    # define name of temporary dummy file
    file_path = pathlib.Path(file).parents[0]
    file_name = pathlib.Path(file).stem
    temp_file = file_path / (file_name + &#39;.bak&#39;)
    # open given original file in read mode  and dummy file in write mode
    with open(file, &#39;r&#39;) as read_obj, open(temp_file, &#39;w&#39;) as write_obj:
        # iterate over list of comments and write them to dummy file as lines
        for line in comments:
            write_obj.write(line)
        # read lines from original file and append them to the dummy file
        for line in read_obj:
            write_obj.write(line)
    # remove original file
    file.unlink()
    # rename dummy file as the original file
    new_extension = temp_file.with_suffix(extension)
    temp_file.rename(new_extension)</code></pre>
</details>
</dd>
<dt id="libratools.lbt_datasets.read_file_paths"><code class="name flex">
<span>def <span class="ident">read_file_paths</span></span>(<span>indir='cwd', extension='.csv', warning=True, suffix=False, suffix_str='')</span>
</code></dt>
<dd>
<div class="desc"><p>Stores paths of files with specified file type in a list; first looks
in current directory before asking user to provide alternative directory
path if none are found.</p>
<h2 id="args">Args</h2>
<dl>
<dt>indir (str, default='cwd'): input directory containing files.</dt>
<dt>extension (str, default='csv'): file extension.</dt>
<dt><strong><code>warning</code></strong> :&ensp;<code>bool default=True</code></dt>
<dd>if warning=True, an informational
message is displayed in case no files are found.</dd>
<dt><strong><code>suffix</code></strong> :&ensp;<code>bool default=False</code></dt>
<dd>if suffix=True, only file paths
ending in suffix_str are returned.</dd>
</dl>
<p>suffix_str (str, ''): suffix.</p>
<h2 id="returns">Returns</h2>
<p>A list of file paths.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_file_paths(indir=&#39;cwd&#39;, extension=&#39;.csv&#39;, warning=True, suffix=False,
                    suffix_str=&#39;&#39;):
    &#34;&#34;&#34;
    Stores paths of files with specified file type in a list; first looks 
    in current directory before asking user to provide alternative directory
    path if none are found.

    Args:
        indir (str, default=&#39;cwd&#39;): input directory containing files.
        extension (str, default=&#39;csv&#39;): file extension.
        warning (bool default=True): if warning=True, an informational 
            message is displayed in case no files are found.
        suffix (bool default=False): if suffix=True, only file paths 
            ending in suffix_str are returned.
        suffix_str (str, &#39;&#39;): suffix.

    Returns:
        A list of file paths.
    &#34;&#34;&#34;
    # search for files in current directory if no indir is provided
    extension = extension.lower()
    extension_cap = extension.capitalize()
    if indir == &#39;cwd&#39;:
        file_paths = [p for p in indir.rglob(f&#39;*{extension}&#39;)]
        num_files = len(file_paths)

        # prompt user for directory path if no none found else store paths
        if num_files == 0:
            print(f&#39;No {extension_cap} files found in current working directory&#39;)

    # check provided directory path exists and read files
    else:
        assert os.path.exists(indir), &#39;Directory path not found.&#39;
        file_paths = [p for p in indir.rglob(f&#39;*{extension}&#39;)]

        # check CSV files exist
        num_files = len(file_paths)
        if num_files == 0 and warning is True:
            print(f&#39;No {extension_cap} files found in {indir}.&#39;)

        if suffix is True:
            files_dir = pathlib.Path(file_paths[0]).parents[0]
            file_stems = [pathlib.Path(p).stem for p in file_paths]
            files = [f + extension for f in file_stems if
                    os.path.splitext(f)[1][-len(suffix_str):] == suffix_str]
            file_paths = [files_dir / f for f in files]

    return file_paths, num_files</code></pre>
</details>
</dd>
<dt id="libratools.lbt_datasets.read_metadata"><code class="name flex">
<span>def <span class="ident">read_metadata</span></span>(<span>file_path, num_comment_lines=3)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns metadata stored as comments in the first few lines of a
BioTracker-generated CSV file as list where each comment is an item
stored as a string.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>file_path</code></strong> :&ensp;<code>str</code></dt>
<dd>path to BioTracker-generated CSV file.</dd>
<dt><strong><code>num_comment_lines</code></strong> :&ensp;<code>int</code>, default=<code>3</code></dt>
<dd>number of lines at the beginning
of file that contain comments.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_metadata(file_path, num_comment_lines=3):
    &#34;&#34;&#34;
    Returns metadata stored as comments in the first few lines of a
    BioTracker-generated CSV file as list where each comment is an item
    stored as a string.

    Args:
        file_path (str): path to BioTracker-generated CSV file.
        num_comment_lines (int, default=3): number of lines at the beginning
            of file that contain comments.
    &#34;&#34;&#34;
    with open(file_path) as file:
        # Read specified number of lines
        metadata = [file.readline() for line in range(num_comment_lines)]

    return metadata</code></pre>
</details>
</dd>
<dt id="libratools.lbt_datasets.read_subdir_paths"><code class="name flex">
<span>def <span class="ident">read_subdir_paths</span></span>(<span>parent_dir='')</span>
</code></dt>
<dd>
<div class="desc"><p>Returns directory paths for all subdirectories in provided directory
path as strings in a list, and number of subdirectories as an integer.S</p>
<h2 id="args">Args</h2>
<p>parent_dir (str, default=''): directory in which to locate all
subdirectories, defaults to current working directory if not
path is provided.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_subdir_paths(parent_dir=&#39;&#39;):
    &#34;&#34;&#34;
    Returns directory paths for all subdirectories in provided directory
    path as strings in a list, and number of subdirectories as an integer.S
    Args:
        parent_dir (str, default=&#39;&#39;): directory in which to locate all
            subdirectories, defaults to current working directory if not
            path is provided.
    &#34;&#34;&#34;
    if parent_dir == &#39;&#39;:
        parent_dir = pathlib.Path.cwd()

    # locate subdirectories
    try:
        subdirs = [parent_dir + file for file in list_dirs(parent_dir)]
    except ValueError:
        print(&#39;Provided directory path not found.&#39;)
    # store directory count as number of recordings
    num_dirs = len(set(subdirs))

    # load each file
    file_paths = []
    for subdir in subdirs:
        file_path, _ = read_file_paths(indir=subdir)
        for file in file_path:
            file_paths.append(file)

    return file_paths, num_dirs</code></pre>
</details>
</dd>
<dt id="libratools.lbt_datasets.read_yaml_as_dict"><code class="name flex">
<span>def <span class="ident">read_yaml_as_dict</span></span>(<span>path)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns yaml dict values.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def read_yaml_as_dict(path):
    &#34;&#34;&#34;
    Returns yaml dict values.
    &#34;&#34;&#34;
    with open(path, &#39;r&#39;) as stream:
        try:
            dic = yaml.safe_load(stream)
        except yaml.YAMLError as exc:
            print(exc)

    return dic</code></pre>
</details>
</dd>
<dt id="libratools.lbt_datasets.save_trajectory_to_csv"><code class="name flex">
<span>def <span class="ident">save_trajectory_to_csv</span></span>(<span>df, f_name='', outdir='', metadata='', extension='.csv', save_msg=True, add_metadata=True, suffix='_processed')</span>
</code></dt>
<dd>
<div class="desc"><p>Saves pandas.DataFrame object as a CSV file and prepends any metadata
provided in a file object.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>df</code></strong> :&ensp;<code>pandas.DataFrame</code></dt>
<dd>dataframe to be saved to file.</dd>
<dt><strong><code>metadata</code></strong> :&ensp;<code>list, str</code></dt>
<dd>metdata to be stored to file, can be either
a string of a list of strings.</dd>
<dt><strong><code>f_name</code></strong> :&ensp;<code>str</code></dt>
<dd>file path.</dd>
<dt><strong><code>outdir</code></strong></dt>
<dd>output directory.</dd>
<dt><strong><code>save_msg</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>prints message to confirm track
has been saved if save_msg=True.</dd>
<dt><strong><code>add_metadata</code></strong> :&ensp;<code>bool</code>, default=<code>True</code></dt>
<dd>adds metadata comments to file
as header if add_metadata=Trueread_file_path.</dd>
</dl>
<p>suffix (str, default='_processed'): suffix to add to the end of file
when saving.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_trajectory_to_csv(df, f_name=&#39;&#39;, outdir=&#39;&#39;, metadata=&#39;&#39;, 
                           extension=&#39;.csv&#39;, save_msg=True, add_metadata=True, 
                           suffix=&#39;_processed&#39;):
    &#34;&#34;&#34;
    Saves pandas.DataFrame object as a CSV file and prepends any metadata
    provided in a file object.
    Args:
        df (pandas.DataFrame): dataframe to be saved to file.
        metadata (list, str): metdata to be stored to file, can be either
            a string of a list of strings.
        f_name (str): file path.
        outdir: output directory.
        save_msg (bool, default=True): prints message to confirm track
            has been saved if save_msg=True.
        add_metadata (bool, default=True): adds metadata comments to file
            as header if add_metadata=Trueread_file_path.
        suffix (str, default=&#39;_processed&#39;): suffix to add to the end of file
            when saving.
    &#34;&#34;&#34;
    # save DataFrame to CSV
    
    f = pathlib.Path(f_name).stem + suffix + extension
    path = outdir / f
    df.to_csv(path, sep=&#39;,&#39;, encoding=&#39;utf-8&#39;,
              index=False)

    # optionally prepend metadata to CSV
    if add_metadata is True:
        prepend_comments_to_csv(path, metadata)

    # optionally confirm saving
    if save_msg:
        if suffix is &#39;_processed&#39;:
            print(f&#39;Processed {pathlib.Path(f_name).stem}{extension} and saved file to disk.\n&#39;)
        else:
            print(f&#39;Merged {pathlib.Path(f_name).stem}{extension} and saved file to disk.&#39;)
    else:
        pass</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="libratools.lbt_datasets.configReader"><code class="flex name class">
<span>class <span class="ident">configReader</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class configReader:
    __conf = None

    @staticmethod
    def config():
        if configReader.__conf is None:  # read only once, lazy
            configReader.__conf = configparser.ConfigParser()
            configReader.__conf.read(CONFIG_PATH)
        return configReader.__conf</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="libratools.lbt_datasets.configReader.config"><code class="name flex">
<span>def <span class="ident">config</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def config():
    if configReader.__conf is None:  # read only once, lazy
        configReader.__conf = configparser.ConfigParser()
        configReader.__conf.read(CONFIG_PATH)
    return configReader.__conf</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Home</h3>
<ul>
<li><code><a title="libratools" href="index.html">libratools</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="libratools.lbt_datasets.check_columns" href="#libratools.lbt_datasets.check_columns">check_columns</a></code></li>
<li><code><a title="libratools.lbt_datasets.dict_to_comments" href="#libratools.lbt_datasets.dict_to_comments">dict_to_comments</a></code></li>
<li><code><a title="libratools.lbt_datasets.extract_comments_as_dict" href="#libratools.lbt_datasets.extract_comments_as_dict">extract_comments_as_dict</a></code></li>
<li><code><a title="libratools.lbt_datasets.find_dir" href="#libratools.lbt_datasets.find_dir">find_dir</a></code></li>
<li><code><a title="libratools.lbt_datasets.get_chunk_number_from_path" href="#libratools.lbt_datasets.get_chunk_number_from_path">get_chunk_number_from_path</a></code></li>
<li><code><a title="libratools.lbt_datasets.list_dirs" href="#libratools.lbt_datasets.list_dirs">list_dirs</a></code></li>
<li><code><a title="libratools.lbt_datasets.load_npz" href="#libratools.lbt_datasets.load_npz">load_npz</a></code></li>
<li><code><a title="libratools.lbt_datasets.load_trajectory" href="#libratools.lbt_datasets.load_trajectory">load_trajectory</a></code></li>
<li><code><a title="libratools.lbt_datasets.prepend_comments_to_csv" href="#libratools.lbt_datasets.prepend_comments_to_csv">prepend_comments_to_csv</a></code></li>
<li><code><a title="libratools.lbt_datasets.read_file_paths" href="#libratools.lbt_datasets.read_file_paths">read_file_paths</a></code></li>
<li><code><a title="libratools.lbt_datasets.read_metadata" href="#libratools.lbt_datasets.read_metadata">read_metadata</a></code></li>
<li><code><a title="libratools.lbt_datasets.read_subdir_paths" href="#libratools.lbt_datasets.read_subdir_paths">read_subdir_paths</a></code></li>
<li><code><a title="libratools.lbt_datasets.read_yaml_as_dict" href="#libratools.lbt_datasets.read_yaml_as_dict">read_yaml_as_dict</a></code></li>
<li><code><a title="libratools.lbt_datasets.save_trajectory_to_csv" href="#libratools.lbt_datasets.save_trajectory_to_csv">save_trajectory_to_csv</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="libratools.lbt_datasets.configReader" href="#libratools.lbt_datasets.configReader">configReader</a></code></h4>
<ul class="">
<li><code><a title="libratools.lbt_datasets.configReader.config" href="#libratools.lbt_datasets.configReader.config">config</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>
